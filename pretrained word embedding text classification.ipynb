{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dense-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "stop=set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D,Dropout\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#!pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "single-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "#############################\n",
    "### Get Data ##\n",
    "\n",
    "train= pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "# concat all\n",
    "df=pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "quality-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "graphic-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spellings(x, spell=spell):\n",
    "    \"\"\"correct the missplled words of a given tweet\"\"\"\n",
    "    x = x.split()\n",
    "    misspelled = spell.unknown(x)\n",
    "    result = map(lambda word : spell.correction(word) if word in  misspelled else word, x)\n",
    "    return \" \".join(result)\n",
    "\n",
    "def tweets_cleaning(x, remove_stop_words=True):\n",
    "    \"\"\"Apply function to a clean a tweet\"\"\"\n",
    "    x = x.lower().strip()\n",
    "    # romove urls\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    x = url.sub(r'',x)\n",
    "    # remove html tags\n",
    "    html = re.compile(r'<.*?>')\n",
    "    x = html.sub(r'',x)\n",
    "    # remove punctuation\n",
    "    x= re.sub('[^a-zA-Z]', ' ', x)\n",
    "    if remove_stop_words:\n",
    "        x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "floating-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "## APPLY the cleaning function to the text column\n",
    "df['cleaned_tweets'] = df['text'].apply(tweets_cleaning)\n",
    "train = df[~df['target'].isna()]\n",
    "X_train, X_val, y_train, y_val = train_test_split(train, train['target'], test_size=0.2, random_state=42)\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "# Limit on the number of features to K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences.\n",
    "# Sequences longer than this will be truncated.\n",
    "# and less than it will be padded\n",
    "MAX_SEQUENCE_LENGTH = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "better-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, train_texts):\n",
    "        self.train_texts = train_texts\n",
    "        self.tokenizer = Tokenizer(num_words=TOP_K)\n",
    "\n",
    "    def train_tokenize(self):\n",
    "        # Get max sequence length.\n",
    "        max_length = len(max(self.train_texts, key=len))\n",
    "        self.max_length = min(max_length, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "        # Create vocabulary with training texts.\n",
    "        self.tokenizer.fit_on_texts(self.train_texts)\n",
    "\n",
    "    def vectorize_input(self, tweets):\n",
    "        # Vectorize training and validation texts.\n",
    "\n",
    "        tweets = self.tokenizer.texts_to_sequences(tweets)\n",
    "        # Fix sequence length to max value. Sequences shorter than the length are\n",
    "        # padded in the beginning and sequences longer are truncated\n",
    "        # at the beginning.\n",
    "        tweets = sequence.pad_sequences(tweets, maxlen=self.max_length, truncating='post', padding='post')\n",
    "        return tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "stable-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CustomTokenizer(train_texts=X_train['cleaned_tweets'])\n",
    "# fit o the train\n",
    "tokenizer.train_tokenize()\n",
    "tokenized_train = tokenizer.vectorize_input(X_train['cleaned_tweets'])\n",
    "tokenized_val = tokenizer.vectorize_input(X_val['cleaned_tweets'])\n",
    "tokenized_test = tokenizer.vectorize_input(test['text'])\n",
    "wordembeddings = gensim.models.KeyedVectors.load_word2vec_format('/home/akshay/Documents/archive/project/GoogleNews-vectors-negative300.bin',binary=True)\n",
    "import tqdm\n",
    "\n",
    "EMBEDDING_VECTOR_LENGTH = 300  # <=200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fluid-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_embedding_matrix(wordembeddings, word_index):\n",
    "    unique_words = len(word_index)\n",
    "    total_words = unique_words + 1\n",
    "    skipped_words = 0\n",
    "    embedding_matrix = np.zeros((total_words, EMBEDDING_VECTOR_LENGTH))\n",
    "    for word, index in tokenizer.tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = wordembeddings[word]\n",
    "        except:\n",
    "            skipped_words = skipped_words + 1\n",
    "            pass\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "changing-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = construct_embedding_matrix(wordembeddings, tokenizer.tokenizer.word_index)\n",
    "model=Sequential()\n",
    "total_words = len(tokenizer.tokenizer.word_index)+1\n",
    "embedding=Embedding(len(tokenizer.tokenizer.word_index)+1, # number of unique tokens\n",
    "                    EMBEDDING_VECTOR_LENGTH, #number of features\n",
    "                    embeddings_initializer=Constant(embedding_matrix), # initialize\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rising-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(embedding)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "optimzer = Adam(clipvalue=0.5) # clip value to avoid the gradient exploding\n",
    "\n",
    "model.compile(optimizer=optimzer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "latter-residence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "191/191 - 8s - loss: 0.5518 - accuracy: 0.7340 - val_loss: 0.4594 - val_accuracy: 0.7951\n",
      "Epoch 2/20\n",
      "191/191 - 6s - loss: 0.4850 - accuracy: 0.7806 - val_loss: 0.4549 - val_accuracy: 0.8037\n",
      "Epoch 3/20\n",
      "191/191 - 7s - loss: 0.4747 - accuracy: 0.7837 - val_loss: 0.4492 - val_accuracy: 0.7984\n",
      "Epoch 4/20\n",
      "191/191 - 7s - loss: 0.4621 - accuracy: 0.7984 - val_loss: 0.4552 - val_accuracy: 0.8122\n",
      "Epoch 5/20\n",
      "191/191 - 7s - loss: 0.4503 - accuracy: 0.7982 - val_loss: 0.4386 - val_accuracy: 0.8109\n",
      "Epoch 6/20\n",
      "191/191 - 7s - loss: 0.4406 - accuracy: 0.8026 - val_loss: 0.4448 - val_accuracy: 0.8194\n",
      "Epoch 7/20\n",
      "191/191 - 7s - loss: 0.4322 - accuracy: 0.8085 - val_loss: 0.4483 - val_accuracy: 0.8162\n",
      "Epoch 8/20\n",
      "191/191 - 7s - loss: 0.4315 - accuracy: 0.8074 - val_loss: 0.4316 - val_accuracy: 0.8102\n",
      "Epoch 9/20\n",
      "191/191 - 7s - loss: 0.4283 - accuracy: 0.8112 - val_loss: 0.4349 - val_accuracy: 0.8142\n",
      "Epoch 10/20\n",
      "191/191 - 7s - loss: 0.4173 - accuracy: 0.8182 - val_loss: 0.5065 - val_accuracy: 0.7886\n",
      "Epoch 11/20\n",
      "191/191 - 7s - loss: 0.4106 - accuracy: 0.8156 - val_loss: 0.4586 - val_accuracy: 0.8089\n",
      "Epoch 12/20\n",
      "191/191 - 7s - loss: 0.4095 - accuracy: 0.8197 - val_loss: 0.4432 - val_accuracy: 0.8168\n",
      "Epoch 13/20\n",
      "191/191 - 7s - loss: 0.4053 - accuracy: 0.8192 - val_loss: 0.4464 - val_accuracy: 0.8175\n",
      "Epoch 14/20\n",
      "191/191 - 7s - loss: 0.3985 - accuracy: 0.8286 - val_loss: 0.4646 - val_accuracy: 0.8030\n",
      "Epoch 15/20\n",
      "191/191 - 7s - loss: 0.3954 - accuracy: 0.8241 - val_loss: 0.4394 - val_accuracy: 0.8142\n",
      "Epoch 16/20\n",
      "191/191 - 7s - loss: 0.3849 - accuracy: 0.8338 - val_loss: 0.4914 - val_accuracy: 0.8116\n",
      "Epoch 17/20\n",
      "191/191 - 7s - loss: 0.3844 - accuracy: 0.8363 - val_loss: 0.4489 - val_accuracy: 0.8194\n",
      "Epoch 18/20\n",
      "191/191 - 7s - loss: 0.3698 - accuracy: 0.8404 - val_loss: 0.4352 - val_accuracy: 0.8221\n",
      "Epoch 19/20\n",
      "191/191 - 7s - loss: 0.3565 - accuracy: 0.8443 - val_loss: 0.4889 - val_accuracy: 0.7991\n",
      "Epoch 20/20\n",
      "191/191 - 7s - loss: 0.3582 - accuracy: 0.8489 - val_loss: 0.4496 - val_accuracy: 0.8181\n",
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f1600c20640>\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "history = model.fit(tokenized_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    validation_data=(tokenized_val,y_val),\n",
    "                    verbose=2)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "compatible-dubai",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>new weapon cause un imaginable destruction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>f  amp  ing things  gishwhes got soaked deluge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>dt  georgegalloway  rt  galloway mayor     col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aftershock back school kick great  want thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>response trauma children addicts develop defen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>2640</td>\n",
       "      <td>crashed</td>\n",
       "      <td>Somewhere</td>\n",
       "      <td>@SmusX16475 Skype just crashed u host</td>\n",
       "      <td>0.0</td>\n",
       "      <td>smusx      skype crashed u host</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>731</td>\n",
       "      <td>attacked</td>\n",
       "      <td>Arundel</td>\n",
       "      <td>Christian Attacked by Muslims at the Temple Mo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>christian attacked muslims temple mount waving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592</th>\n",
       "      <td>5131</td>\n",
       "      <td>fatal</td>\n",
       "      <td>New South Wales, Australia</td>\n",
       "      <td>Man charged over fatal crash near Dubbo refuse...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>man charged fatal crash near dubbo refused bai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6740</th>\n",
       "      <td>9657</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#usNWSgov Severe Weather Statement issued Augu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>usnwsgov severe weather statement issued augu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>2361</td>\n",
       "      <td>collapsed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Great British &amp;lt;b&amp;gt;Bake&amp;lt;/b&amp;gt; Off's ba...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>great british  lt b gt bake lt  b gt  back dor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1523 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       keyword                    location  \\\n",
       "2644  3796   destruction                         NaN   \n",
       "2227  3185        deluge                         NaN   \n",
       "5448  7769        police                          UK   \n",
       "132    191    aftershock                         NaN   \n",
       "6845  9810        trauma       Montgomery County, MD   \n",
       "...    ...           ...                         ...   \n",
       "1835  2640       crashed                   Somewhere   \n",
       "506    731      attacked                    Arundel    \n",
       "3592  5131         fatal  New South Wales, Australia   \n",
       "6740  9657  thunderstorm                         NaN   \n",
       "1634  2361     collapsed                         NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "2644  So you have a new weapon that can cause un-ima...     1.0   \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...     0.0   \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...     1.0   \n",
       "132   Aftershock back to school kick off was great. ...     0.0   \n",
       "6845  in response to trauma Children of Addicts deve...     0.0   \n",
       "...                                                 ...     ...   \n",
       "1835              @SmusX16475 Skype just crashed u host     0.0   \n",
       "506   Christian Attacked by Muslims at the Temple Mo...     1.0   \n",
       "3592  Man charged over fatal crash near Dubbo refuse...     1.0   \n",
       "6740  #usNWSgov Severe Weather Statement issued Augu...     1.0   \n",
       "1634  Great British &lt;b&gt;Bake&lt;/b&gt; Off's ba...     0.0   \n",
       "\n",
       "                                         cleaned_tweets  \n",
       "2644        new weapon cause un imaginable destruction   \n",
       "2227  f  amp  ing things  gishwhes got soaked deluge...  \n",
       "5448  dt  georgegalloway  rt  galloway mayor     col...  \n",
       "132   aftershock back school kick great  want thank ...  \n",
       "6845  response trauma children addicts develop defen...  \n",
       "...                                                 ...  \n",
       "1835                    smusx      skype crashed u host  \n",
       "506   christian attacked muslims temple mount waving...  \n",
       "3592  man charged fatal crash near dubbo refused bai...  \n",
       "6740   usnwsgov severe weather statement issued augu...  \n",
       "1634  great british  lt b gt bake lt  b gt  back dor...  \n",
       "\n",
       "[1523 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "impressive-necessity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshay/mlbook/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "y_pred= model.predict_classes(tokenized_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "second-martin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[797,  77],\n",
       "       [200, 449]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "military-headquarters",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181221273801708"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-casino",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
